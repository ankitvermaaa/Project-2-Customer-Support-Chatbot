{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9zd4DDRdn11CMfZHmx1ZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitvermaaa/Project-2-Customer-Support-Chatbot/blob/main/Project_2_Customer_Support_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVmXjp1MgFAR",
        "outputId": "c641df9a-a02b-4242-d1b5-6458512ce965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n"
      ],
      "metadata": {
        "id": "EbBI1eS6gOaz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')        # fixes lemmatizer error\n",
        "nltk.download('punkt_tab')      # fixes tokenization error in Colab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eUhipOPgxjX",
        "outputId": "142624da-5f23-47f7-9de3-aba7fc4fa1f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Intent\": [\"Order Status\", \"Return Policy\", \"Product Info\"],\n",
        "    \"Example Query\": [\n",
        "        \"Where is my order #12345?\",\n",
        "        \"How can I return a product?\",\n",
        "        \"Does this phone support fast charging?\"\n",
        "    ],\n",
        "    \"Response\": [\n",
        "        \"Your order is out for delivery. ðŸšš\",\n",
        "        \"You can return products within 15 days via our online portal.\",\n",
        "        \"Yes, this phone supports fast charging âš¡.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "474jbhG-g1C6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # lowercase all words\n",
        "    text = re.sub(r'#\\d+', '', text)  # remove order numbers like #12345\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "YCU9UNU_g5LO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Processed_Query'] = df['Example Query'].apply(preprocess)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Processed_Query'])"
      ],
      "metadata": {
        "id": "Lhb4IL-Pg_dg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query):\n",
        "    query_processed = preprocess(query)\n",
        "    query_vec = vectorizer.transform([query_processed])\n",
        "    similarity = cosine_similarity(query_vec, X)\n",
        "    idx = similarity.argmax()\n",
        "    return df.iloc[idx]['Response']"
      ],
      "metadata": {
        "id": "sMin0oAShD19"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ¤– Customer Support Chatbot (type 'exit' to quit)\")\n",
        "while True:\n",
        "    user_input = input(\"Customer: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Bot: Thank you! Have a great day ðŸ˜Š\")\n",
        "        break\n",
        "    print(\"Bot:\", get_response(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k6Vufl1hGj1",
        "outputId": "5a4f337c-46d7-4d44-8165-e3d81c188ecc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Customer Support Chatbot (type 'exit' to quit)\n",
            "Customer: exit\n",
            "Bot: Thank you! Have a great day ðŸ˜Š\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BZG_QWMhJOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}